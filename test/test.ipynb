{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91459817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac4c2166",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../olmocr_sample.pdf\", \"rb\") as f:\n",
    "    pdf_base64 = base64.b64encode(f.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ea8ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:8000/ocr\"\n",
    "payload = {\n",
    "    \"file_base64\": pdf_base64,\n",
    "    \"start_page\": 1,\n",
    "    \"end_page\": 25\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c00830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.post(url, json=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8d50008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON: {'ocr_results': {'1': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"olmOCR: Unlocking Trillions of Tokens in PDFs with Vision Language Models\\\\n\\\\nJake Poznanski  Aman Rangapur\\\\nJon Borchard  Jason Dunkelberger  Regan Huff  Daniel Lin  Christopher Wilhelm\\\\nKyle Lo  Luca Soldaini\\\\n\\\\nAllen Institute for AI, Seattle, USA  {jakep|kylel|lucas}@allenai.org  ♦ indicates core contributors.\\\\n\\\\nAbstract\\\\n\\\\nPDF documents have the potential to provide trillions of novel, high-quality tokens for training language models. However, these documents come in a diversity of types with differing formats and visual layouts that pose a challenge when attempting to extract and faithfully represent the underlying content for language model use. Traditional open source tools often produce lower quality extractions compared to vision language models (VLMs), but reliance on the best VLMs can be prohibitively costly (e.g., over $6,240 USD per million PDF pages for GPT-4o) or infeasible if the PDFs cannot be sent to proprietary APIs. We present olmOCR, an open-source toolkit for processing PDFs into clean, linearized plain text in natural reading order while preserving structured content like sections, tables, lists, equations, and more. Our toolkit runs a fine-tuned 7B vision language model (VLM) trained on olmOCR-mix-0225, a sample of 260,000 pages from over 100,000 crawled PDFs with diverse properties, including graphics, handwritten text and poor quality scans. olmOCR is optimized for large-scale batch processing, able to scale flexibly to different hardware setups and can convert a million PDF pages for only $176 USD. To aid comparison with existing systems, we also introduce olmOCR-Bench, a curated set of 1,400 PDFs capturing many content types that remain challenging even for the best tools and VLMs, including formulas, tables, tiny fonts, old scans, and more. We find olmOCR outperforms even top VLMs including GPT-4o, Gemini Flash 2 and Qwen-2.5-VL. We openly release all components of olmOCR: our fine-tuned VLM model, training code and data, an efficient inference pipeline that supports vLLM and SGLang backends, and benchmark olmOCR-Bench.\"}', '2': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"1 Introduction\\\\n\\\\nAccess to clean, coherent textual data is a crucial component in the life cycle of modern language models (LMs). During model development, LMs require training on trillions of tokens derived from billions of documents (Soldaini et al., 2024; Penedo et al., 2024; Li et al., 2024); errors from noisy or low fidelity content extraction and representation can result in training instabilities or even worse downstream performance (Penedo et al., 2023; Li et al., 2024; OLMo et al., 2024). During inference, LMs are often prompted with plain text representations of relevant document context to ground user prompts; for example, consider information extraction (Kim et al., 2021) or AI reading assistance (Lo et al., 2024) over a user-provided document and cascading downstream errors due to low quality representation of the source document.\\\\n\\\\nWhile the internet remains a valuable source of textual content for language models, large amounts of content are not readily available through web pages. Electronic documents (e.g., PDF, PS, DjVu formats) and word processing files (e.g., DOC, ODT, RTF) are widely-used formats to store textual content. However, these formats present a unique challenge: unlike modern web standards, they encode content to facilitate rendering on fixed-size physical pages, at the expense of preserving logical text structure. For example, consider the PDF format, which originated as a means to specify how digital documents should be printed onto physical paper. As seen in Figure 2, PDFs store not units of text—headings, paragraphs, or other meaningful prose elements—but single characters alongside their spacing, placement, and any metadata used for visual rendering on a page. As more and more documents became digital, users have relied this file format to create trillions of documents (PDF Association staff, 2015); yet, these documents remain difficult to leverage in LM pipelines because PDFs lack basic structure necessary for coherent prose, such as ground truth reading order.\\\\n\\\\nFaithful content extraction and representation of digitized print documents has long been of interest, with early research efforts in the 1950s, and first commercial optical character recognition (OCR) tools debuting in the late 1970s (Mori et al., 1992). The release of Tesseract in 2006 represented a significant milestone, as the first high-quality, open-source OCR toolkit (Smith, 2013). The current landscape of PDF extraction toolkits...\"}', '3': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce **olmOCR**, a general-purpose context extraction and linearization toolkit to convert PDFs or images of documents into clean plain text suitable for language model development. Our contributions in this work are as follows:\\\\n\\\\n- **Data.** We create **olmOCR-mix-0225**, a collection of 260,000 crawled PDF pages paired with their OCR output by GPT-4o, that we use to train our models. These documents represent a diverse set of publicly available PDFs, with a skew towards academic papers, public domain books, legal documents, brochures, and more.\\\\n\\\\n- **Benchmark.** We develop **olmOCR-Bench**, a comprehensive benchmark for evaluating document extraction tools. Unlike existing evaluation methods, **olmOCR-Bench** uses simple, natural binary rules, like software unit tests, that enable direct comparisons across different OCR systems without relying on fuzzy gold reference matching or LLM-as-judge for evaluation. The benchmark covers 1,400 PDF pages with over 7,000 unit-test cases spanning diverse document types.\\\\n\\\\n---\\\\n\\\\n1With batch pricing, at $1.25 USD (input) and $5.00 USD (output) per million tokens in Feb 2025.\"}', '4': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"• **Model and Code.** We fine-tune Qwen2-VL-7B-Instruct (Wang et al., 2024b) on olmOCR-mix-0225, producing olmOCR-7B-0225-preview. We package our VLM in the olmOCR Python toolkit, written to scale efficiently from one to hundreds of GPUs using SGLang (Zheng et al., 2024) and vLLM (Kwon et al., 2023) inference engines. olmOCR achieves state-of-the-art performance on our benchmark, even outperforming Qwen-2.5-VL-7B while remaining more cost-effective than existing alternatives, including commercial APIs; olmOCR can produce high-quality plain text at less than $176 per million PDF pages.\\\\n\\\\n• **Downstream Use.** We demonstrate real-world impact by applying olmOCR to process the 7.9M original PDFs in peS2o (Soldaini and Lo, 2023), a widely-used corpus of linearized scientific articles used in language model pretraining. We show that training on the newly extracted data called olmOCR-peS2o can improve language model pretraining, observable even in downstream benchmark performance.\\\\n\\\\n## 2 Creating and Training on olmOCR mix\\\\n\\\\nWe face two challenges in data acquisition necessary for developing a VLM for our task: (1) acquiring a large, diverse set of PDFs and (2) obtaining their linearized plain text as supervision targets.\\\\n\\\\n### 2.1 Crawling PDFs\\\\n\\\\n| Source                  | Unique docs | Total pages |\\\\n|-------------------------|-------------|-------------|\\\\n| Web crawled PDFs        | 96,929      | 240,940     |\\\\n| Internet Archive books  | 5,896       | 17,701      |\\\\n| **Total**               | **102,825** | **258,641** |\\\\n\\\\nTable 1 olmOCR-mix-0225 composition by source. Web crawled PDFs are sampled from a set of over 240 million documents crawled from public websites. Books in the Internet Archive set are in the public domain.\\\\n\\\\nWe randomly sample PDFs from an internal dataset of 240 million PDFs crawled from public internet sites, as well as PDFs of public domain books sourced from the Internet Archive. While the web crawled set is often born-digital documents, PDFs from the Internet Archive consist of image scans. We then perform a set of filters: Using the Lingua package (Emond, 2025), we identify and filter out non-English documents. Further, we remove any document that failed to be parsed by pypdf, contains spam keywords, is a fillable form, or whose text is too short.\\\\\\\\(^2\\\\\\\\) We then sampled (up to) three pages uniformly at random from each PDF. We summarize the data distribution in Tables 1 and 2.\\\\n\\\\n### 2.2 Generating Linearized Plain Text\\\\n\\\\nObtaining supervision targets for converting PDF to plain text presents a fundamental challenge. First, human annotation is prohibitively expensive and can be error-prone. Second, existing tools that extract content from PDF internals don’t work on document images, but also don’t provide reliable ground truth due to extraction errors from brittle heuristics. In this work, we turned to data generation using GPT-4o to reliably convert PDF pages to linearized plain text.\\\\\\\\(^3\\\\\\\\)\\\\n\\\\n\\\\\\\\(^2\\\\\\\\)An implementation of these heuristics is available on GitHub: /olmocr/filter/filter.py#L14-L112.\\\\n\\\\n\\\\\\\\(^3\\\\\\\\)In October 2024, we evaluated several leading VLMs for data generation. Gemini 1.5 was eliminated due to frequent RECITATION errors (though this was resolved by February 2025), GPT-4o mini produced excessive hallucinations, and Claude Sonnet 3.5 was cost-prohibitive. We selected gpt-4o-2024-08-06 as it offered the optimal balance of accuracy, reliability, and cost-efficiency in batch mode.\\\\n\\\\n| Document type | Fraction |\\\\n|---------------|----------|\\\\n| Academic      | 55.9%    |\\\\n| Brochure      | 11.2%    |\\\\n| Legal         | 10.2%    |\\\\n| Books         | 6.8%     |\\\\n| Table         | 5', '5': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yet, GPT-4o does not produce sufficiently high-fidelity plain text on its own; for high-density pages or complex layouts, we found it is prone to omitting content, rewriting or completing content in a manner unfaithful to the original, or captioning images when not instructed to do so. To help guide GPT-4o generations, we experiment with augmenting the visual input (PDF page raster) with text blocks and position information extracted from the page. As mentioned in A, we refer to this approach as DOCUMENT-ANCHORING.\\\\n\\\\nWe use the pypdf (PyPDF, 2012–2025) library to extract a representation of the page’s structure from the PDF’s internal data. We note that this representation is highly noisy: reading order is not preserved and main content is interwoven with boilerplate text and PDF rendering-related artifacts. We sample blocks from this long extraction to add to the prompt until maximum input length is exceeded; we prioritize text blocks and images which are located at the start and end of the document.\\\\n\\\\nFinally, we instruct GPT-4o to respond with structured output to our requests. We report the full JSON schema in Appendix E.1. This forces the model to first extract page metadata, such as language, page orientation, and presence of tables, before generating the text of the page in a natural reading order. This format allows for more efficient processing of output; further, we found it crucial to ensure that GPT-4o does not generate captions of images when no text is present on the page. Overall, we find DOCUMENT-ANCHORING indeed improves the output quality of GPT-4o according to our benchmark (§3) reported in Table 4.\\\\n\\\\n2.3 Model Training\\\\n\\\\nFine-tuning While DOCUMENT-ANCHORING could be used to prompt any language model, its performance may depend on the model (Table 4), making it best suited as a data generation technique. This leaves open the question whether a smaller, specialized VLM can be as accurate as optimized prompting of a larger, general-purpose model.\\\\n\\\\nStarting from a Qwen2-VL-7B-Instruct checkpoint, we fine-tune olmOCR-7B-0225-preview on olmOCR-mix-0225. Training is implemented using Hugging Face’s transformers library (Wolf et al., 2020). We use an effective batch size of 4, learning rate of 1e-6, AdamW optimizer, and a cosine annealing schedule for 10,000 steps (roughly 1.2 epochs). We use single node with 8 x NVIDIA H100 (80GB) GPUs. A single training run took 16 node hours, with all training experiments totaling 365 node hours.\\\\n\\\\nDuring fine-tuning, we slightly alter the DOCUMENT-ANCHORING prompt, removing some instructions and shrinking the image size so that PDF pages are rendered to a maximum dimension of 1024 pixels on the longest edge. The simplified text prompt is in Appendix E.2. The prompt is capped to 6,000 characters, so a typical prompt uses about 1,000 tokens to encode a page image, 1,800 tokens for the anchor text, for about 3,000 total input tokens. Each training example was truncated to 8,192 tokens to cover cases when the prompt was unusually large. Loss was masked so only the final response tokens participated in the loss calculation.\\\\n\\\\nWe keep the same structured JSON output that was present in the outputs of olmOCR-mix-0225. More training evaluations are noted in Appendix C.\\\\n\\\\n3 Building olmOCR-Bench\\\\n\\\\nWe develop olmOCR-Bench to systematically evaluate PDF linearization and content extraction performance across diverse tools and models. olmOCR-BENCH operates by assessing a series of predefined pass-or-fail “unit-tests”—Given an input whole PDF, does the plain text output satisfy a specific property or contain a specific element? Each test is designed to be simple, unambiguous, and deterministically machine-verifiable. This avoids reliance on model-based evaluators which can be biased towards favoring their own generations (Panickssery et al., 2024). It also avoids use of soft metric (e.g., edit distance, ROUGE) comparisons against reference text which might fail to reveal fine-grained yet semantically important content extraction errors, as is the case with incorrect math formulas (e.g., $x^i$ vs $x_i$). olmOCR-BENCH comprises 1,402 distinct PDF documents derived from diverse source repositories, covered by 7', '6': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We designed 5 distinct test categories, each designed to assess specific aspects of linearization and context extraction performance. We describe the test definitions and scoring methods below:\\\\n\\\\n- **Text Presence**: Verifies that a text segment (typically spanning 1-3 sentences) is correctly identified within the plain text output. Soft/fuzzy matching is allowed, as well as specifying if the text must be in the first \\\\\\\\( N \\\\\\\\) or last \\\\\\\\( N \\\\\\\\) characters of the document. Case-sensitive by default.\\\\n\\\\n- **Text Absence**: Verifies that a text segment is successfully excluded from the plain text output. This category primarily targets peripheral content such as recurring headers, footers, and pagination markers. Soft/fuzzy matching is allowed, as well as specifying if the text must be in the first \\\\\\\\( N \\\\\\\\) or last \\\\\\\\( N \\\\\\\\) characters of the document. Not case-sensitive by default.\\\\n\\\\n- **Natural Reading Order**: Verifies the order between two text segments. For instance, on a PDF with multiple news articles on one page, we can test for whether the first sentence of the first article appears after the heading of that article; yet such tests can be designed to not penalize for the order of the articles themselves. Soft matching is allowed, case-sensitive by default.\\\\n\\\\n- **Table Accuracy**: Checks that the plain text output contains a table with a cell with a given value, and that its neighboring cells have certain properties. For instance, one can validate this page has a table with a cell containing “4.5%” and above that is a cell containing “2.4%”. Both Markdown and HTML based tables are supported, though many cases depend on `rowspan` and `colspan` information being preserved, which is possible only in HTML based tables.\\\\n\\\\n- **Math Formula Accuracy**: Checks that the plain text output contains a given math equation. We render a reference \\\\\\\\LaTeX\\\\\\\\ equation using KaTeX in a headless browser and extract all rendered symbols and their (visual) bounding boxes. Then we check if a matching collection of symbols, with the same relative orientations, exists anywhere in the final OCR document. For instance, if searching for \\\\\\\\( f(x) = \\\\\\\\int_{-3}^{3} x^2 \\\\\\\\, dx \\\\\\\\) on a page, we look for an equation where \\\\\\\\( f \\\\\\\\) appears to the left of a \\\\\\\\( x \\\\\\\\), \\\\\\\\( x \\\\\\\\) appears to the left of \\\\\\\\( dx \\\\\\\\), 3 appears above \\\\\\\\( -3 \\\\\\\\), and so on. This is similar to the method described by Wang et al. (2025), but ours is simpler due to the test being Pass/Fail only.\\\\n\\\\n- **Baseline**: Each PDF document by default also receives a baseline or default test case, which checks that some plain text output containing alphanumeric characters was actually produced for that page, that such output does not have a string of repeating \\\\\\\\( N \\\\\\\\) grams at the end (longer than 30), and that the output does not contain any characters from the Chinese, Japanese, or Emoji Unicode charsets.\\\\\\\\(^5\\\\\\\\)\\\\n\\\\nIn all cases where text is compared, we perform basic string normalization, such as converting `<br>`s to newlines, normalizing all whitespace to single ASCII spaces, removing Markdown bold/italics, normalizing quotes/hyphens to ASCII, and converting all Unicode to NFC format.\\\\n\\\\n\\\\\\\\(^5\\\\\\\\)This is to test for common failure cases of models, such as accidentally switching languages and generating repeated outputs. The handful of pages which do legitimately contain such charsets are manually flagged and excluded from such test conditions.\\\\n\\\\n| Presence | Absence | Read Order | Table | Formula | Total tests |\\\\n|----------|---------|------------|-------|---------|-------------|\\\\n| arXiv Math (AM) | - | - | - | - | 2,927 |\\\\n| Old Scans Math (OSM) | - | - | - | - | 458 |\\\\n| Tables (TA) | - | - | - | 1,020 | 1,020 |\\\\n| Old Scans (OS) | 279 | 70 | 177 | - | - | 526 |\\\\n| Headers Footers (HF) | - | 753 | - | - | 753 |\\\\n| Multi Column (MC) | - | - | 884 | - | - | 884 |\\\\n| Long Tiny Text (LTT) | 442 | - | - | - | 442 |\\\\n| **Total Tests** | 721 | 823 |', '7': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Sourcing Documents and Creating Tests\\\\n\\\\nWe define 7 distinct document types that we found olmOCR (or its earlier iterations) often struggled to process and defined custom acquisition strategies for each (described below). We removed documents that both contained PII and were not meant for public dissemination; prompt in Appendix F.2.2. We also decontaminate against documents that appear in olmOCR-mix-0225 via URL level deduplication. To scale creation of test cases over these documents, we combined manual design and review with prompting GPT-4o; further details and prompts are in Appendix F. Visualize sample documents in Appendix F.3.\\\\n\\\\n- **arXiv Math (AR)** We downloaded a recent set of papers from the math subset of arXiv, selecting manuscripts with a single TeX source file and corresponding rendered PDF. To select a candidate \\\\\\\\( \\\\\\\\text{\\\\\\\\LaTeX} \\\\\\\\) expression from a page to use in a test, we (1) ran olmOCR to identify candidate pages with TeX, (2) match pages back to original TeX source, and (3) validate matched TeX rendering compatibility with KaTeX.\\\\n\\\\n  We manually verify the final set of test cases to exclude instances where custom macros produce renderings that deviate from standard \\\\\\\\( \\\\\\\\text{\\\\\\\\LaTeX} \\\\\\\\) and to split multi-part equations into smaller test cases.\\\\n\\\\n- **Old Scans Math (OSM)** We crawl old, public domain math textbooks from the Internet Archive\\\\\\\\(^6\\\\\\\\), extracting random pages from these documents. We similarly use olmOCR to find candidate pages with formulas, but this time manually annotate each formula on the page to use as test cases.\\\\n\\\\n- **Tables (TA)** We sampled more documents from the same internal crawled PDF repository used to create olmOCR-mix-0225 and filtered to those which had tables using a simple prompt with Gemini-Flash-2.0. On pages with tables, we prompted Gemini-Flash-2.0 for the relationships between randomly chosen cells. We manually reviewed those tests for accuracy.\\\\n\\\\n- **Old Scans (OS)** We sampled historical letters and typewritten documents with existing human transcriptions from the Library of Congress\\\\\\\\(^7\\\\\\\\) digital archives. We then wrote a small script to generate Natural Reading Order cases consisting of sentences that were naturally before or after one another in the original human transcriptions. We manually added test cases to cover some headers/footers which should have been excluded from any OCR version of these documents. All of the test cases then underwent a second pass of human review for accuracy.\\\\n\\\\n- **Headers Footers (HF)** We visually sample documents from our internal crawled PDF repository to find documents with multi-column layouts and multiple articles on one page. We use Claude-Sonnet-3.7 to render those pages to HTML, and from that HTML, we extract text segments before/after one another. We manually review each entry for accuracy. We purposely select simple text blocks from coherent regions of the document, and avoid including any math formulas, superscripts, or subscripts in these tests.\\\\n\\\\n- **Long Tiny Text (LTT)** We crawled documents from the Internet Archive containing a large amount of dense, small print on a single page. Such documents include pages from a dictionary or pages of references from academic papers. We then generate test cases using Gemini-Flash-2.0 and verify them manually.\\\\n\\\\n3.3 Scoring\\\\n\\\\nWe run each of the PDF pages across each of our tools and methods to produce a markdown or plain text document. As all tests are Pass/Fail, we simply report percentage of tests passed, macro-averaged by document type. We evaluate each of the tests to get a percentage correct score for each test source (plus the default baseline tests). The final score for each tool is the average of the percentage across each test source. This\\\\n\\\\n---\\\\n\\\\n\\\\\\\\(^6\\\\\\\\)https://archive.org\\\\n\\\\n\\\\\\\\(^7\\\\\\\\)https://crowd.loc.gov\"}', '8': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"captures the difficulty we faced at times of finding and validating enough cases from each source, but we roughly feel that each source represents an important capability for an OCR system to have.\\\\n\\\\n\\\\\\\\[\\\\n\\\\\\\\text{Overall score} = \\\\\\\\frac{1}{N} \\\\\\\\sum_{s \\\\\\\\in \\\\\\\\text{Document sources}} \\\\\\\\text{Score}(s)\\\\n\\\\\\\\]\\\\n\\\\n4 Evaluating olmOCR\\\\n\\\\nFirst, we evaluate olmOCR on olmOCR-Bench against a range of linearization tools and VLMs (Section §4.1). We then quantify the usefulness olmOCR for language modeling by continued pretraining on an OLMo 2 checkpoint (OLMo et al., 2024) on content extracted and linearized with our toolkit (Section §4.2).\\\\n\\\\nAdditional evaluations, studying how faithful olmOCR is to its teacher model (Section §C.1), and pairwise ELO comparison (Section §C.2) are available in the appendix.\\\\n\\\\n4.1 olmOCR-Bench Results\\\\n\\\\nFrom Table 4, we see that olmOCR significantly outperforms both the best commercial dedicated OCR tool (Mistral) as well as both GPT-4o, its teacher model, and Qwen 2.5 VL, which is an update to Qwen 2 VL, which was the base model for olmOCR-7B-0225-preview. We note that we developed olmOCR-Bench after training olmOCR-7B-0225-preview to prevent unfairly iterating on the benchmark before comparing with other methods. Qualitatively, olmOCR produces significantly cleaner plain text than specialized open-source tools (visualized in Appendix G).\\\\n\\\\n| Model                  | AR  | OSM | TA  | OS  | HF  | MC  | LTT | Base | Overall |\\\\n|------------------------|-----|-----|-----|-----|-----|-----|-----|------|---------|\\\\n| GOT OCR                | 52.7| 52.0| 0.2 | 22.1| 93.6| 42.0| 29.9| 94.0 | 48.3 ± 1.1 |\\\\n| Marker v1.7.5          | 76.0| 57.9| 57.6| 27.8| 84.9| 72.9| 84.6| 99.1 | 70.1 ± 1.1 |\\\\n| MinerU v1.3.10         | 75.4| 47.4| 60.9| 17.3| 96.6| 59.0| 39.1| 96.6 | 61.5 ± 1.1 |\\\\n| Mistral OCR API        | 77.2| 67.5| 60.6| 29.3| 93.6| 71.3| 77.1| 99.4 | 72.0 ± 1.1 |\\\\n| GPT-4o (No Anchor)     | 51.5| 75.5| 69.1| 40.9| 94.2| 68.9| 54.1| 96.7 | 68.9 ± 1.1 |\\\\n| GPT-4o (Anchored)      | 53.5| 74.5| 70.0| 40.7| 93.8| 69.3| 60.6| 96.8 | 69.9 ± 1.1 |\\\\n| Gemini Flash 2 (No Anchor) | 32.1| 56.3| 61.4| 27.8| 48.0| 58.7| 84.4| 94.0 | 57.8 ± 1.1 |\\\\n| Gemini Flash 2 (Anchored) | 54.5| 56.1| 72.1| 34.2| 64.7| 61.5| 71.5| 95.6 | 63.8 ± 1.2 |\\\\n| Qwen 2 VL (No Anchor)  | 19.', '9': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in Table 5, replacing the original peS2o tokens extracted via Grobid + rules with those processed used olmOCR results in a +1.3 percentage point average improvement on widely-reported LM benchmark tasks, including MMLU (Hendrycks et al., 2021), ARC, DROP (Dua et al., 2019), HellaSwag (Zellers et al., 2019), NaturalQuestions (Kwiatkowski et al., 2019), WinoGrande (Sakaguchi et al., 2019).\\\\n\\\\n| peS2o version | Average | MMLU | ARC | DROP | HSwag | NQ | WinoG |\\\\n|---------------|---------|------|-----|------|-------|----|-------|\\\\n| Grobid + rules (Soldaini and Lo, 2023) | 53.9 | 61.1 | 75.0 | 42.3 | 57.4 | 29.4 | 58.3 |\\\\n| olmOCR-peS2o | 55.2 | 61.1 | 76.4 | 43.7 | 62.6 | 29.1 | 58.0 |\\\\n\\\\nTable 5 Comparison on OLMo 2 (OLMo et al., 2024) downstream evaluation tasks of OLMo-2-7B-1124 on 50B of original peS2o tokens vs 50B tokens from the same source PDFs but processed with olmOCR.\\\\n\\\\n4.3 Cost Evaluation\\\\n\\\\nFinally, when considering real-world use, cost efficiency is just as important as performance. We present a summary of inference costs in Table 6. To contextualize the value of olmOCR, at 1,000 tokens per page, to process all of peS2o PDFs can already cost $10.3M in H100 usage. In comparison, Mistral OCR is a commercial API tool specializing in this task, yet is over five times more expensive, making it even more prohibitive to use for language modeling. See Appendix B for details on pricing and cost calculations.\\\\n\\\\n| Model | Hardware | Tokens/sec | Pages/USD | Cost per million pages |\\\\n|-------|----------|------------|-----------|------------------------|\\\\n| GPT-4o | API | - | 80 | $12,480 |\\\\n| | Batch | - | 160 | $6,240 |\\\\n| Marker v1.7.5 (Force OCR) | H100 | 332 | 674 | $1484 |\\\\n| Mistral OCR | API | - | 1,000 | $1,000 |\\\\n| MinerU | L40S | 238 | 1,678 | $596 |\\\\n| Gemini Flash 2 | API | - | 2,004 | $499 |\\\\n| | Batch | - | 4,008 | $249 |\\\\n| olmOCR | L40S | 906 | 5,697 | $176 |\\\\n| | H100 | 3,050 | 5,632 | $178 |\\\\n\\\\nTable 6 Inference cost comparison against other OCR methods. NVIDIA L40S estimated at $0.79 per hour, H100 80GB estimated at $2.69 per hour. We measured a 12% retry rate for olmOCR. Full cost breakdown in Appendix B.\\\\n\\\\n5 Related Work\\\\n\\\\nTools and Models for Linearizing PDFs to Plain Text. Many tools have existed for this task, some are parsers of born-digital PDF internals while others are OCR tools on top of image rasters of PDF pages. As machine learning matured, more people started developing models that automate this PDF parsing; examples include LayoutLM (Xu et al., 2020) and VILA (Lin et al., 2024). Tools have been developed around use of these models, including PaperMage (Lo et al., 2023b), or have updated to include their own custom trained models, like Grobid (gro, 2008–2025). Commercial API providers began integrating VLMs with document processing capabilities: OpenAI introduced GPT-4 Vision (Open', '10': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"on form understanding with typewritten content, SROIE (Huang et al., 2019) concentrates on information extraction from scanned receipts, and RVL-CDIP (Harley et al., 2015) contains scanned documents. However, these datasets exhibit significant limitations: they are predominantly domain specific, targeting narrow document categories with constraint formatting variations, while our approach leverages a diverse corpus spanning multiple domains and document types. Additionally, traditional benchmarks often focus on isolated extraction tasks (e.g., exclusively evaluating tables with PubTabNet (Zhong et al., 2020), or mathematical formulas with specialized detection frameworks (Zhong et al., 2021)), whereas our benchmark evaluates performance across a comprehensive spectrum of extraction challenges. Further, their evaluation method is brittle, typically relying on exact string matching against predefined gold-standard tokens (which makes it difficult to compare methods that produce different tokenizations). In contrast, our unit-test-style evaluation framework enables equitable assessment across diverse implementations regardless of their underlying tokenization mechanisms, providing a more generalizable evaluation paradigm for document understanding systems.\\\\n\\\\n6 Conclusion\\\\n\\\\nWe introduce olmOCR, an open-source toolkit for converting PDF documents into clean plain text. Our approach combines DOCUMENT-ANCHORING, a novel prompting technique that leverages available metadata in born-digital PDFs, with a fine-tuned 7B parameter vision language model to achieve results competitive with closed commercial solutions at a fraction of the cost. We openly release our training set olmOCR-mix-0225 to enable others to further develop their own VLMs.\\\\n\\\\nTo rigorously evaluate the system, we developed olmOCR-BENCH, a benchmark of 7,010 test instances across 1,403 PDFs. It includes Pass/Fail unit tests for text presence, reading order, tables, formulas, and baseline functionality. The documents span categories from scientific papers to historical manuscripts, enabling robust assessment across diverse linearization and context extraction challenges.\\\\n\\\\nOur released efficient inference pipeline contains everything needed to start converting anything from single documents to million-page archives of PDFs. We hope olmOCR’s ability to efficiently process millions of documents will help unlock new sources of training data for language models, particularly from high-quality PDF documents that are currently underrepresented in existing datasets that rely heavily solely on crawled web pages.\\\\n\\\\nAcknowledgments\\\\n\\\\nThis work would not be possible without the support of our colleagues at AI2. We thank Byron Bischoff, Aaron Sarnat, Huy Tran, Sam Skjonsberg, Eric Marsh, and Chris Newell for help setting up the live demo; Taira Anderson, Sruthi Sreeram for program management support; Will Smith and Crystal Nam for legal guidance; Michael Schnitz, Caitlin Wittlif and Carissa Schoenick for various indirect support. We also thank Benjamin Charles Germain Lee for helpful feedback and suggestions on evaluation and potential use cases.\\\\n\\\\nReferences\\\\n\\\\nGROBID. https://github.com/kermitt2/grobid, 2008–2025.\"}', '11': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023. https://arxiv.org/abs/2308.12966.\\\\n\\\\nShuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wang, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-VL technical report. arXiv [cs.CV], February 2025.\\\\n\\\\nAdrien Barbaresi. Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction. In Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 122–131. Association for Computational Linguistics, 2021. https://aclanthology.org/2021.acl-demo.15.\\\\n\\\\nJanek Bevendorff, Benno Stein, Matthias Hagen, and Martin Potthast. Elastic ChatNoir: Search Engine for the ClueWeb and the Common Crawl. In Leif Azzopardi, Allan Hanbury, Gabriella Pasi, and Benjamin Piwowarski, editors, Advances in Information Retrieval. 40th European Conference on IR Research (ECIR 2018), Lecture Notes in Computer Science, Berlin Heidelberg New York, March 2018. Springer.\\\\n\\\\nCody Blakeney, Mansheep Paul, Brett W. Larsen, Sean Owen, and Jonathan Frankle. Does your data spark joy? performance gains from domain upsampling at the end of training, 2024. https://arxiv.org/abs/2406.03476.\\\\n\\\\nLukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical understanding for academic documents, 2023. https://arxiv.org/abs/2308.13418.\\\\n\\\\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs, 2019. https://arxiv.org/abs/1903.00161.\\\\n\\\\nPatrick Emond. Lingua-py: Natural language detection for python, 2025. https://github.com/pemistahl/lingua-py. Accessed: 2025-01-06.\\\\n\\\\nGemini Team. Gemini: A family of highly capable multimodal models, 2025. https://arxiv.org/abs/2312.11805.\\\\n\\\\nGoogle. Explore document processing capabilities with the gemini API. https://web.archive.org/web/20250224064040/https://ai.google.dev/gemini-api/docs/document-processing?lang=python, 2025. Accessed: 2025-2-23.\\\\n\\\\nAaron Grattafori, Abhimanyu Dubey, Abhinav Jauhari, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gwergerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong', '12': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gouget, Virginie Do, Vish Vogeti, Vitor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipsos, Aadiya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Alhuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Binge Bo, Bu Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Lissavetska, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Lissavetska, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Flores, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi Zhang, Gunak Lakshminarayanan, Hakan Inan, Hamid Shojaianzari, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Ruddolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damla, Igor Molybo, Igor Tufanov, Ilia Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Gebski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matsosich, Kaushik Veeraraghavan, Kelly Michele, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, L', '13': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1516–1520. IEEE, 2019.\\\\n\\\\nGeewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision, 2021. https://api.semanticscholar.org/CorpusID:250924870.\\\\n\\\\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_00276. https://aclanthology.org/Q19-1026/.\\\\n\\\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Liannin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with paged-attention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.\\\\n\\\\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:14200–14282, 2024.\\\\n\\\\nJi Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2024. https://arxiv.org/abs/2312.07533.\\\\n\\\\nQian Liu, Xiaosen Zheng, Niklas Muennighoff, Guangtao Zeng, Longxu Dou, Tianyu Pang, Jing Jiang, and Min Lin. Regmix: Data mixture as regression for language model pre-training. arXiv preprint arXiv:2407.01492, 2024.\\\\n\\\\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic scholar open research corpus. In Dan Jurafsky, Joyce Chai, Natalie Schuler, and Joel Tetradre, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969–4983, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.447. https://aclanthology.org/2020.acl-main.447/.\\\\n\\\\nKyle Lo, Zejiang Shen, Benjamin Newman, Joseph Chang, Russell Author, Erin Bransom, Stefan Candra, Yoganand Chandrasekhar, Regan Huff, Bailey Kuehl, Amanpreet Singh, Chris Wilhelm, Angele Zamarron, Marti A. Hearst, Daniel Weld, Doug Downey, and Luca Soldaini. PaperMage: A unified toolkit for processing, representing, and manipulating visually-rich scientific documents. In Yansong Feng and Els Lefever, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 495–507, Singapore, December 2023a.', '14': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hanish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James Validad Miranda, Jacob Daniel Morrison, Tyler C. Murray, Denny Jin, Shino Jomoto, Billie Jon, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondrak, Andrew Kondrich, Aris Konstantidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ika Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makajnu, Kim Malliacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavy, Paul McMillan, Jake McNeil, David Medina, Alok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reichihiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emi Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Froehl, Raoul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tseak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weimann, Akila Welhinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. https://arxiv.org/abs/2303.08774.\\\\n\\\\nArjun Panickser, Samuel R. Bowman, and Shi Feng. Llm evaluators recognize and favor their own generations, 2024. https://arxiv.org/abs/2404.13076.\\\\n\\\\nVik Paruchuri. Marker: Convert pdf to markdown + json quickly with high', '15': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Outperforming curated corpora with web data, and web data only. ArXiv, abs/2306.01116, 2023. https://api.semanticscholar.org/CorpusID:259063761.\\\\n\\\\nGuilherme Penedo, Hynek Kydlíček, Anton Lozhkov, Margaret Mitchell, Colin A Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:30811–30849, 2024.\\\\n\\\\nPyPDF. Pypdf: A pure-python pdf library. https://github.com/py-pdf/pypdf, 2012–2025. Accessed: 2025-01-06.\\\\n\\\\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. https://arxiv.org/abs/1907.10641.\\\\n\\\\nZejiang Shen, Kyle Lo, Lucy Lu Wang, Bailey Kuehl, Daniel S. Weld, and Doug Downey. VILA: Improving structured content extraction from scientific PDFs using visual layout groups. Transactions of the Association for Computational Linguistics, 10:376–392, 2022. doi: 10.1162/tacl_a_00466. https://aclanthology.org/2022.tacl-1.22/.\\\\n\\\\nRay W Smith. History of the tesseract OCR engine: what worked and what didn’t. In Richard Zanibbi and Bertrand Coisnon, editors, Document Recognition and Retrieval XX, volume 8658, page 865802. SPIE, February 2013. doi: 10.1117/12.2010051.\\\\n\\\\nLuca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report, Allen Institute for AI, 2023. ODC-By, https://github.com/allenai/peS2o.\\\\n\\\\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Arthur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hanneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024. https://arxiv.org/abs/2402.00159.\\\\n\\\\nBin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liquan Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, and Conghui He. Mineru: An open-source solution for precise document content extraction, 2024a. https://arxiv.org/abs/2409.18839.\\\\n\\\\nBin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Bo Zhang, and Conghui He. Image over text: Transforming formula recognition evaluation with character detection matching, 2025. https://arxiv.org/abs/2409.03643.\\\\n\\\\nPeng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin', '16': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2024. https://arxiv.org/abs/2312.07104.\\\\n\\\\nXu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. Image-based table recognition: data, model, and evaluation. In European conference on computer vision, pages 564–580. Springer, 2020.\\\\n\\\\nYuxiang Zhong, Xianbiao Qi, Shanjun Li, Dengyi Gu, Yihao Chen, Peiyang Ning, and Rong Xiao. 1st place solution for icdar 2021 competition on mathematical formula detection, 2021. https://arxiv.org/abs/2107.05534.\"}', '17': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Methodology\\\\n\\\\nApproach Many end-to-end OCR models, such as GOT Theory 2.0 (Wei et al., 2024) and Nougat (Blecher et al., 2023), exclusively rely on rasterized pages to convert documents to plain text; that is, they process images of the document pages as input to autoregressively decode text tokens. This approach, while offering great compatibility with image-only digitization pipelines, misses the fact that most PDFs are born-digital documents, thus already contain either digitized text or other metadata that would help in correctly linearizing the content.\\\\n\\\\nFigure 3 Example of how DOCUMENT-ANCHORING works for a typical page. Relevant image locations and text blocks get extracted, concatenated, and inserted into the model prompt. When prompting a VLM for a plain text version of the document, the anchored text is used in conjunction with the rasterized image of a page.\\\\n\\\\nIn contrast, the olmOCR pipeline leverages document text and metadata. We call this approach DOCUMENT-ANCHORING. Figure 3 provides an overview of our method; DOCUMENT-ANCHORING extracts coordinates of salient elements in each page (e.g., text blocks and images) and injects them alongside raw text extracted from the PDF binary file. Crucially, the anchored text is provide as input to any VLM alongside a rasterized image of the page.\\\\n\\\\nOur approach increases the quality of our content extraction. We apply DOCUMENT-ANCHORING when prompting GPT-4o to collect silver training samples, when fine-tuning olmOCR-7B-0225-preview, and when performing inference with the olmOCR toolkit.\\\\n\\\\nImplementation DOCUMENT-ANCHORING processes PDF document pages via the pypdf (PyPDF, 2012–2025) library to extract a representation of the page’s structure from the underlying PDF. All of the text blocks and images in the page are extracted, including position information. Starting with the most relevant text blocks and images, these are sampled and added to the prompt of the VLM, up to a defined maximum character limit. This extra information is then available to the model when processing the document.\\\\n\\\\nOverall, we find that using prompts constructed using DOCUMENT-ANCHORING results in significantly fewer hallucinations. Prompting with just the page image was prone to models completing unfinished sentences, or to invent larger texts when the image data was ambiguous. Finally, while DOCUMENT-ANCHORING helps with quality on born-digital documents, our pipeline maintains high performance on documents that do not\\\\n\\\\n---\\\\n\\\\n8 We prioritize text blocks and images which are located at the start and end of the document.\\\\n9 We use a character limit for convenience and speed, but during training or inference, if a page’s prompt exceeds the model’s token limit, we just regenerate it with exponentially decreasing character limits until it is suitable.\"}', '18': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"have any digital metadata encoded in them. In these cases, the model will not have the benefit of seeing the internal structure of the PDF document, instead relying on just the rasterized image of a page to process the underlying document.\\\\n\\\\nB Cost Estimates of PDF Extraction Systems\\\\n\\\\nTo estimate prices, we use rates provided by RunPod\\\\\\\\textsuperscript{10} as of February 2025. It prices a single on-demand NVIDIA L40S GPU at $0.79 USD per hour, and NVIDIA H100 80GB SXM at $2.69 USD per hour. Using these rates, costs (in USD) were computed as follows:\\\\n\\\\n- **GPT-4o**: We evaluated GPT-4o in February 2025. We tested 1288 pages, which resulted in 3,093,315 input tokens at 833,599 output tokens. Priced at $2.50 per million input tokens and $10.00 per million output tokens, it resulted in a total of $16.07. Batch processing is priced at half of the cost, $8.03.\\\\n\\\\n- **Mistral OCR**: As of May 2025, Mistral prices their OCR service at $1 per 1,000 pages, regardless of number of generated tokens.\\\\n\\\\n- **MinerU**: We run the toolkit (version 1.3.10) on a single NVIDIA L40S GPU. It processed 1,288 pages in 58 minutes 22 seconds, costing $0.767.\\\\n\\\\n- **Marker**: We run marker v1.7.5 using the marker command line with the `force_ocr` flag on 10,000 pages selected randomly from olmOCR-mix-0225. This took 5 hours, 31 minutes on an H100 node with 1 GPU, resulting in a price of $14.84 for 10,000 pages.\\\\\\\\textsuperscript{11}\\\\n\\\\n- **Gemini Flash 2.0**: As of February 2025, it is priced $0.10 per 1 million input tokens, and $0.40 per 1 million output tokens. In our testing over the same 1,288 pages used to evaluate GPT-4o, it cost in $0.643.\\\\n\\\\n- **olmOCR**: We tested the launch version of olmOCR on both L40S and H100 GPUs. On L40s, it processed 1,288 test pages in 17 minutes, 10 seconds. The effective throughput of the model was 906 output tokens per second, plus a 12% reties rate. Overall, we estimate its costs at $0.226. On H100, olmOCR generates 3,050 output tokens per second, resulting in a runtime of 5 minutes 7 seconds, for a cost of $0.229.\\\\n\\\\nC Evaluation of Trained Models\\\\n\\\\n![Figure 4 Validation Loss - Web PDFs](image1)\\\\n\\\\n![Figure 5 Validation Loss - Internet Archive Books](image2)\\\\n\\\\nWe track validation loss during training of olmOCR-7B-0225-preview against a development subset of olmOCR-mix-0225 during fine-tuning; Figure 4 and Figure 5, show the loss curves for both the web PDFs and the Internet Archive books subsets. LoRA resulted in higher loss values compared to full fine-tuning, which we use for the final model.\\\\n\\\\nTo set hyperparameters and make other decisions during development, we relied on manual side-by-side evaluation as shown in Figure 6. A random selection of 20 to 50 documents were processed using two different\\\\n\\\\n\\\\\\\\textsuperscript{10}https://www.runpod.io\\\\n\\\\n\\\\\\\\textsuperscript{11}The `force_ocr` option is more expensive, but results in better performance. The Marker authors are working on improving performance on large GPUs with multiple workers which should lower this cost.\"}', '19': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"methods, and were displayed side by side along with the render of the document page. We also open source our evaluation tool to support qualitative inspection of this visually-rich data.\\\\n\\\\nC.1 Alignment with Teacher Model\\\\n\\\\nTo compare the output of \\\\\\\\texttt{olmOCR-7B-0225-preview} to the GPT-4o silver data in \\\\\\\\texttt{olmOCR-mix-0225}, we build a document similarity metric which splits a document into words, uses Hirschberg’s algorithm to align those words, and counts what proportion match.\\\\n\\\\nWe report alignment scores in Table 7. Overall, we find that \\\\\\\\texttt{olmOCR-7B-0225-preview} has good alignment, 0.875 on average, with its teacher model. To calibrate this result, we also report GPT-4o self-alignment score of 0.954, which is simply from calling the model again; imperfect alignment here is due to resampling differences. In fact, we find that our model actually better mimics the content extraction and linearization of GPT-4o than its smaller counterpart GPT-4o mini.\\\\n\\\\nWhen partitioning scores in low, medium, and high alignment buckets (Table 8), we find that most documents parsed with \\\\\\\\texttt{olmOCR} have medium to high alignment with GPT-4o. Increasing temperature unsurprisingly leads to a wider distribution of alignment scores, as noted by the increase of low matches for $\\\\\\\\tau = 0.8$.\\\\n\\\\n| Model               | Temperature $\\\\\\\\tau$ | Alignment |\\\\n|---------------------|-------------------|-----------|\\\\n| GPT-4o (self-alignment) | 0.1               | 0.954     |\\\\n| GPT-4o mini         | 0.1               | 0.833     |\\\\n| \\\\\\\\texttt{olmOCR}     | 0.8               | 0.859     |\\\\n| \\\\\\\\texttt{olmOCR}     | 0.1               | 0.875     |\\\\n\\\\nTable 7 Page-weighted alignment between GPT-4o, GPT-4o mini, and our fine-tuned model. We find that \\\\\\\\texttt{olmOCR-7B-0225-preview} is more consistent with respect to its teacher than GPT-4o mini. Note that GPT-4o does not achieve a perfect alignment against itself due to the probabilistic nature of autoregressive decoding.\"}', '20': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Name                        | Low match | Medium match | High match |\\\\n|-----------------------------|-----------|--------------|------------|\\\\n| GPT-4o (self alignment)     | 38        | 218          | 965        |\\\\n| GPT-4o mini                 | 214       | 478          | 529        |\\\\n| olmOCR ($\\\\\\\\tau = 0.1$)       | 158       | 363          | 700        |\\\\n| olmOCR ($\\\\\\\\tau = 0.8$)       | 195       | 390          | 636        |\\\\n\\\\nTable 8  Match-up between olmOCR and different models compared to the olmOCR-mix-0225 dataset. Low match indicates < 70% alignment, Medium match is 70-95% alignment, High match is >95% alignment.\\\\n\\\\nC.2 Intrinsic Human Evaluation\\\\n\\\\nExperimental setup  To compare olmOCR against other common OCR methods, we collected pairwise human judgments of plain text produced by the three top ML-based PDF linearization tools—Marker, MinerU, and GOT-OCR 2.0—and calculating ELO ratings.\\\\n\\\\nTo create our evaluation set, we sample 2,017 new PDFs from the same distribution as used to create olmOCR-mix-0225 and run each PDF through olmOCR and the linearization tools mentioned above. All other linearization tools were installed from either PyPI or Github according to their publicly available instructions as of January 14th, 2025. GOT-OCR 2.0 was configured in ‘format’ mode, but otherwise all comparisons were done against default settings.\\\\n\\\\nWe then sampled 2,000 comparison pairs (same PDF, different tool). We asked 11 data researchers and engineers at Ai2 to assess which output was the higher quality representation of the original PDF, focusing on reading order, comprehensiveness of content and representation of structured information. The user interface used is similar to that in Figure 6. Exact participant instructions are listed in Appendix C.3.\\\\n\\\\nEvaluation results  We collected a total of 452 judgments where a participant expressed a preference between two models (the remaining 1,548 pairs were either skipped for being too similar, or marked as invalid). On average, this is 75 judgments per pair of tools. We calculate ELO ratings starting from a base of 1500 and report the average of 100 simulations to avoid ordering effects in ELO calculations; for 95% confidence intervals, we use bootstrapping with 5000 resamples.\\\\n\\\\nWe visualize our results in Figure 7. olmOCR achieves an ELO score over 1800, far exceeding all other PDF linearization tools.\"}', '21': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.3 ELO Evaluation Instructions\\\\n\\\\nIn Section C.2, we asked participants to compare the output of various common OCR tools against olmOCR. Participants were given the instructions below, and presented with a document page, and the output of two random tools. They could then select which output was better, or select ‘Both Good’, ‘Both Bad’, or ‘Invalid PDF’ any of which would not count the comparison in the ELO ranking.\\\\n\\\\nInstructions to participants\\\\n\\\\n| Compare the text in the two fields, and select which one better represents the contents of the document. |\\\\n| REMINDER: This is not about \\\\\"the most faithful OCR\\\\\", but \\\\\"this OCR output seems really useful for training LMs\\\\\" |\\\\n| - Does the text capture all of the meaningful content in the document in a natural order? |\\\\n| - Are the words correct (no weird incorrect words or split words) |\\\\n| - Is the whitespace sensible? |\\\\n| - Is the useless header/footer content removed? |\\\\n| - Do the tables/equations look okay? |\\\\n\\\\nThere is not a strict preference between Markdown and LaTeX, most importantly you should evaluate it on the text content, not which method was used to format it.\\\\n\\\\nIf you are not sure, or the document is in a language other than english, you can skip that entry, or mark \\\\\"both good\\\\\" \\\\\"both bad\\\\\", \\\\\"invalid pdf\\\\\".\\\\n\\\\nELO data\\\\n\\\\nWe compute pairwise win/loss statistics between models to estimate relative performance under head-to-head comparisons. As shown in Table 9, olmOCR consistently outperforms other models such as MARKER, GOTOCR, and MINERU, with the highest win rate of 71.4% against MINERU.\\\\n\\\\nTable 9 Pairwise Win/Loss Statistics Between Models\\\\n\\\\n| Model Pair       | Wins | Win Rate (%) |\\\\n|------------------|------|--------------|\\\\n| olmOCR vs. MARKER| 49/31| 61.3         |\\\\n| olmOCR vs. GOTOCR| 41/29| 58.6         |\\\\n| olmOCR vs. MINERU| 55/22| 71.4         |\\\\n| MARKER vs. MINERU| 53/26| 67.1         |\\\\n| MARKER vs. GOTOCR| 45/26| 63.4         |\\\\n| GOTOCR vs. MINERU| 38/37| 50.7         |\\\\n| Total            | 452  |              |\\\\n\\\\nD Deploying olmOCR\\\\n\\\\nD.1 Inference Pipeline\\\\n\\\\nTo efficiently convert millions of documents, we develop the olmOCR pipeline using SGLang (Zheng et al., 2024) as the inference engine. The pipeline batches documents into work items of around 500 pages each. Each work item is then queued to run on a worker with access to a GPU for inference. Optionally, workers can coordinate using a shared cloud bucket\\\\\\\\(^\\\\\\\\text{12}\\\\\\\\), allowing for batch jobs that scale from single nodes to hundreds of nodes without the need for complicated queue management.\\\\n\\\\n\\\\\\\\(^{12}\\\\\\\\)We use Amazon Simple Storage Service (S3), but other cloud providers or network storage solutions could be easily used.\"}', '22': '{\"primary_language\":\"en\",\"is_rotation_valid\":false,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We summarize our efforts by comparing operational costs of olmOCR against other API and local models in Table 6. Overall, we find olmOCR to be significantly more efficient than other pipelines. It is over 32 times cheaper than GPT-4o in batch mode; compared to other purpose-built pipelines and models, olmOCR is over 6 times cheaper than MinerU, and $1/3^{rd}$ of the cost of marker.\\\\n\\\\nTo balance maintaining high GPU utilization while also ensuring work items are completed quickly, each worker queues up inference for all PDF pages in a work item simultaneously, and then waits until the SGLang server has no more pending requests before proceeding to another work item in the queue.\\\\n\\\\n## D.2 Increasing Robustness\\\\n\\\\nWe implement several heuristics to improve reliability of olmOCR without compromising its throughput.\\\\n\\\\n**Prompt format** During inference, we use the same abbreviated prompt described in Section §2.3. This keeps the test time examples looking the same as what the model was trained on. If the additional tokens generated by DOCUMENT-ANCHORING cause the overall prompt to exceed 8,192 tokens, then we continue regenerating the DOCUMENT-ANCHORING tokens with exponentially lower character limits until the overall prompt is of acceptable length.\\\\n\\\\n**Retries** Unlike when we created olmOCR-mix-0225, we do not enforce a specific JSON schema during inference on our fine-tuned model. This is for two reasons: first, we find that open source tools designed to force decode a sequence into a particular schema are unreliable, and that enforcing a schema which is even slightly off from what the model expects can cause generations to go out-of-domain or collapse into repetitions. Second, and most importantly, we note that, since the model was extensively fine-tuned on the structured output, it reliably adheres to the required schema without constraints. For the rare cases when JSON parsing fails, we simply retry generating from the same input sequence.\\\\n\\\\n**Rotations** The output JSON schema includes fields for is_rotation_valid and rotation_correction. During inference, olmOCR pipeline reads these two fields and if is_rotation_valid is set to true it will rotate the page by the amount specified in rotation_correction and reprocess the page.\\\\n\\\\n**Decoding** In developing olmOCR, the most common failure we experience is outputs degenerating into endless repetitions of the same token, line, or paragraph. This failure is caught automatically when the model’s output either exceeds the maximum context length, or does not validate against our JSON schema. We find that increasing generation temperature from $\\\\\\\\tau = 0.1$ up to $\\\\\\\\tau = 0.8$ reduces the likelihood of repetitions occurring. Further, we modify olmOCR to reprocess failed pages up to N times, falling back to a plain text-based PDF extraction if the pipeline repeatedly fails. This last mitigation is aided by the fact that DOCUMENT-ANCHORING randomly samples which anchors to include in the prompt; thus, resampling can sometimes help the page process correctly by removing potentially problematic meta tokens.\\\\n\\\\nWe note that one one limitation of this approach is that, if retries occur often, the total generation throughput could be significantly reduced. Further, letting generations repeat up to maximum sequence length uses significant memory within SGLang. In future work, we plan to detect repeated generations sooner than at the maximum context length limit, and abort promptly.\\\\n\\\\n### E olmOCR-mix-0225 and olmOCR-7B-0225-preview Prompts\\\\n\\\\n#### E.1 olmOCR-mix-0225 construction prompt for GPT-4o\\\\n\\\\nThe prompt below was used to create the silver dataset, which we refer to as olmOCR-mix-0225 throughout the paper. This dataset consists of structured outputs generated by GPT-4o, using images of PDF pages along with additional layout-aware textual features produced by our DOCUMENT-ANCHORING pipeline. We use this synthetic data to fine-tune our model.\\\\n\\\\nIn this prompt, the placeholder `{base_text}` is replaced with the structured layout-aware text extracted from the PDF using DOCUMENT-ANCHORING. The prompt instructs GPT-4o to output the natural reading-order\"}', '23': '{\"primary_language\": null, \"is_rotation_valid\": true, \"rotation_correction\": 0, \"is_table\": false, \"is_diagram\": false, \"natural_text\": null}', '24': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E.2 olmOCR-7B-0225-preview prompt\\\\n\\\\nThe prompt below is used to draw responses from our fine-tuned model during inference. As before, the placeholder \\\\\\\\{base_text\\\\\\\\} is replaced with the output of the DOCUMENT-ANCHORING pipeline i.e., layout-aware textual features extracted from the PDF page.\\\\n\\\\nBelow is the image of one page of a document, as well as some raw textual content that was previously extracted for it. Just return the plain text representation of this document as if you were reading it naturally. Do not hallucinate.\\\\n\\\\n{base_text}\\\\n\\\\nE.3 olmOCR-mix-0225 Classification Prompt\\\\n\\\\nThe prompt and structured schema below was used to classify a sample of documents from olmOCR-mix-0225 as reported in Table 2.\\\\n\\\\nThis is an image of a document page, please classify it into one of the following categories that best overall summarizes its nature: academic, legal, brochure, slideshow, table, diagram, or other. Also determine the primary language of the document and your confidence in the classification (0-1).\\\\n\\\\n```python\\\\nclass DocumentCategory(str, Enum):\\\\n    ACADEMIC = \\\\\"academic\\\\\"\\\\n    LEGAL = \\\\\"legal\\\\\"\\\\n    BROCHURE = \\\\\"brochure\\\\\"\\\\n    SLIDESHOW = \\\\\"slideshow\\\\\"\\\\n    TABLE = \\\\\"table\\\\\"\\\\n    DIAGRAM = \\\\\"diagram\\\\\"\\\\n    OTHER = \\\\\"other\\\\\"\\\\n\\\\nclass DocumentClassification(BaseModel):\\\\n    category: DocumentCategory\\\\n    language: str\\\\n    confidence: float\\\\n```\\\\n\\\\nE.4 olmOCR-mix-0225 PII Prompt\\\\n\\\\nWe implemented comprehensive prompting for detecting personally identifiable information (PII) in the documents while cleaning the olmOCR-mix-0225:\\\\n\\\\nYou are a document analyzer that identifies Personally Identifiable Information (PII) in documents.\\\\nYour task is to analyze the provided document image and determine:\\\\n1. Whether the document is intended for public release or dissemination (e.g., research paper, public report, etc.)\\\\n2. If the document contains any PII\\\\n\\\\nFor PII identification, follow these specific guidelines:\\\\nIDENTIFIERS FOR PII:\\\\nThe following are considered identifiers that can make information PII:\\\\n- Names (full names, first names, last names, nicknames)\\\\n- Email addresses\\\\n- Phone numbers\"}', '25': '{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PII THAT MUST CO-OCCUR WITH AN IDENTIFIER:\\\\nThe following types of information should ONLY be marked as PII if they occur ALONGSIDE an identifier (commonly, a person\\'s name):\\\\n- Addresses (street address, postal code, etc.)\\\\n- Biographical Information (date of birth, place of birth, gender, sexual orientation, race, ethnicity, citizenship/immigration status, religion)\\\\n- Location Information (geolocations, specific coordinates)\\\\n- Employment Information (job titles, workplace names, employment history)\\\\n- Education Information (school names, degrees, transcripts)\\\\n- Medical Information (health records, diagnoses, genetic or neural data)\\\\n\\\\nPII THAT OCCURS EVEN WITHOUT AN IDENTIFIER:\\\\nThe following should ALWAYS be marked as PII even if they do not occur alongside an identifier:\\\\n- Government IDs (Social Security Numbers, passport numbers, driver’s license numbers, tax IDs)\\\\n- Financial Information (credit card numbers, bank account/routing numbers)\\\\n- Biometric Data (fingerprints, retina scans, facial recognition data, voice signatures)\\\\n- Login information (ONLY mark as PII when a username, password, and login location are present together)\\\\n\\\\nIf the document is a form, then only consider fields which are filled out with specific values as potential PII.\\\\nIf this page does not itself contain PII, but references documents (such as curriculum vitae, personal statements) that typically contain PII, then do not mark it as PII.\\\\nOnly consider actual occurrences of the PII within the document shown.\"}'}}\n"
     ]
    }
   ],
   "source": [
    " print(\"JSON:\", resp.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
